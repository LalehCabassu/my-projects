{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was create by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part I - Preliminaries (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In these hands-on exercices we will be focusing on manipulating Resilient Distributed Datasets (RDDs). We introduce `map`, `mapValues`, `reduce`, `reduceByKey`, `aggregateByKey`, `filter` and `join` to transform, aggregate, and connect datasets. Each function can be stringed together to do more complex tasks.\n",
    "\n",
    "The first part deals with movieLens dataset. These datasets will be used to build a movie' recommendation system based on Non Negative Matrix Factorization (NMF) methodology (Part II). In this part we work together as __Q & A__ (Questions and Answers).\n",
    "\n",
    "The second part (data processing of textual dataset) is your home work to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-50795e6836c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set up spark environment (Using Spark Local Mode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# set up spark environment (Using Spark Local Mode)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `SparkContext` launches a web UI, that displays useful information about the application. \n",
    "\n",
    "- A list of scheduler stages and tasks\n",
    "- A summary of RDD sizes and memory usage\n",
    "- Environmental information\n",
    "- Information about the running executors\n",
    "\n",
    "We can access this interface by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens dataset\n",
    "\n",
    "We will work with ratings from users on movies, collected by [MovieLens](https://movielens.org). This dataset is pre-loaded under `data/movielens/`. For quick testing of your code, you may want to use a smaller dataset under `data/movielens/medium`, which contains 1 million ratings from 6000 users on 4000 movies.\n",
    "\n",
    "We will use two files from this dataset: `ratings.dat` and `movies.dat`. All ratings are contained in the file `ratings.dat` and are in the following format:\n",
    "\n",
    "```\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "```\n",
    "The movie information is in the file `movies.dat` and is in the following format:\n",
    "\n",
    "```\n",
    "MovieID::Title::Genres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's start with the data. Loading the dataset:\n",
    "- [MovieLens 1M Dataset](http://grouplens.org/datasets/movielens/1m/ml-1m.zip) - 1 million ratings from 6000 users on 4000 movies.\n",
    "- [MovieLens 20M Dataset](http://grouplens.org/datasets/movielens/20m/ml-20m.zip) - 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. \n",
    "- [MovieLens latest Dataset](http://grouplens.org/datasets/movielens/20m/ml-20m.zip) - 22 million ratings and 580,000 tag applications applied to 33,000 movies by 240,000 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    ">Define two functions `parseRating` and `parseMovie` that parse a rating and a movie record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseRating(line):\n",
    "    \"\"\" Parse a rating record in MovieLens format UserID::MovieID::Rating::Timestamp\n",
    "    Args:\n",
    "        line (str): a line in the ratings dataset in the form of UserID::MovieID::Rating::Timestamp\n",
    "    Returns:\n",
    "        tuple: (UserID, MovieID, Rating)\n",
    "    \"\"\"\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseMovie(line):\n",
    "    \"\"\" Parse a movie record in MovieLens format MovieID::Title::Genres\n",
    "    Args:\n",
    "        entry (str): a line in the movies dataset in the form of MovieID::Title::Genres\n",
    "    Returns:\n",
    "        tuple: (MovieID, Title, Genres)\n",
    "    \"\"\"\n",
    "    fields = line.split(\"::\")\n",
    "    return int(fields[0]), fields[1], fields[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__\n",
    "\n",
    ">Create two RDDs by \n",
    "* reading a file with <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile\">`textFile`</a>\n",
    "* using the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map\">`map`</a> transformation operation with the above defined functions to create them\n",
    "* assigning them a name with <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=setname#pyspark.RDD.setName\">`setName`</a> (e.g. `movies` and `ratings` respectively).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to MovieLens dataset\n",
    "movieLensHomeDir=\"data/movielens/medium/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# movies is an RDD of (movieID, title, genre)\n",
    "moviesRDD = .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ratings is an RDD of (userID, movieID, rating)\n",
    "ratingsRDD = .... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ - In these lines of code, we are creating the `moviesRDD` and `ratingsRDD` variables (technically RDDs) and we are pointing to files (on your local PC). Spark’s lazy nature means that it doesn’t automatically compile your code. Instead, it waits for some sort of action occurs that requires some calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">Make your first transformation to get the number of ratings, distinct users and movies from the ratings RDD. (see the various native operations on <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=count#pyspark.RDD\">RDDs</a> in the doc) <br/>\n",
    ">Display several elements of each created RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numRatings = ....\n",
    "numUsers   = ....\n",
    "numMovies  = ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    "\n",
    ">Define two new RDDs containing only the movies for genre _Comedy_ and all movies that have _Comedy_ among other genres.<br/>\n",
    ">Use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter\">`filter`</a> function which return a new RDD containing only the elements that satisfy a predicate.<br/>\n",
    ">Use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.subtract\">`subtract`</a> function to retreive the movies that have  _Comedy_ in their genres but not only (That is the elements of the second RDD minus the ones in the first). Count them and exhibit a few of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    "\n",
    ">Investigate the different movies genres. Warning: Multiples genres should not be seen as new genres! For this:\n",
    "* separate the genres by delimiter '|' using  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap\">`flatMap`</a>\n",
    "* use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct\">`distinct`</a> function which return a new RDD containing the distinct elements in this RDD.\n",
    "\n",
    ">Count the number of different genres and print them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=left src=\"files/images/pyspark-page23.svg\" width=500 height=500 />\n",
    "</a>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    ">Get the average of all of the ratings. There are different solutions:\n",
    "* use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean\">`mean`</a> built-in function\n",
    "* use the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce\">`reduce`</a> function and define you own function for summing two ratings\n",
    "\n",
    ">Compare these approaches in terms of execution time by using `iPython`'s magic command <a href=\"https://ipython.org/ipython-doc/3/interactive/magics.html#magic-timeit\">`timeit`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Question 7__\n",
    "\n",
    "> Get the average rating for each movie and user.<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Get top-$n$ movies with highest average ratings.<br/>\n",
    "> Get top-$n$ Movies with highest average ratings and more than 500 reviews.<br/>\n",
    "> Save results on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Compute the sparsity of the rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 10__\n",
    "\n",
    ">Get the rating distribution and plot histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LibSVM dataset (home work)\n",
    "\n",
    "\n",
    "__Question 1__\n",
    "\n",
    "> Examine the output of MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to ionosphere LibSVM\n",
    "LibSVMHomeDir=\"data/LibSVM/\"\n",
    "LibName=\"ionosphere.txt\"\n",
    "#LibName=\"rcv1_train.binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, LibSVMHomeDir + LibName).setName(\"LibSVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__\n",
    "\n",
    ">Count the the number of examples, the number of features, and the sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">Create your own LibSVM Reader file (you can use the number of features to simplify writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
